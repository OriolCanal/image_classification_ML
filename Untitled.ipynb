{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647dd6b7-962d-4b10-8d51-4a4de5b1b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89549a2-2e84-40da-94be-3c1633a5c65f",
   "metadata": {},
   "source": [
    "First of all, let's define the paths where our taining and validation datasets are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ab5fc2-4d76-44a4-b645-4d7397d12cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ocanal/Desktop/UriCanal/master_machine_learning/advanced_machine_learning/project1_image_clasification'\n",
    "train_dir = os.path.join(path, \"train\")\n",
    "validation_dir = os.path.join(path, \"validation\")\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dca42e8-1bc0-440b-9136-e9d8c5547112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Dataset for the Training data\n",
    "train = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,  # Directory where the Training images are located\n",
    "    labels = 'inferred', # Classes will be inferred according to the structure of the directory\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['airplane', 'automobile', 'bird', \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],\n",
    "    batch_size = 16,    # Number of processed samples before updating the model's weights\n",
    "    image_size = (256, 256), # Defining a fixed dimension for all images\n",
    "    shuffle = True,  # Shuffling data\n",
    "    seed = seed,  # Random seed for shuffling and transformations\n",
    "    validation_split = 0, # We don't need to create a validation set from the training set\n",
    "    crop_to_aspect_ratio = True # Resize images without aspect ratio distortion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a978fbe-51dc-4342-b573-7833547e225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset for the Test data\n",
    "validation = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_dir,  \n",
    "    labels = 'inferred', \n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['airplane', 'automobile', 'bird', \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],\n",
    "    batch_size = 16,    \n",
    "    image_size = (256, 256), \n",
    "    shuffle = True,  \n",
    "    seed = seed,  \n",
    "    validation_split = 0, \n",
    "    crop_to_aspect_ratio = True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b3eee-5cb8-437c-87b8-a17dd047f696",
   "metadata": {},
   "source": [
    "Let's explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff61750c-7379-4bb6-a3ed-4c9d32a467b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n",
      "\n",
      "Validation Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining Dataset:', train)\n",
    "print('\\nValidation Dataset:', validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931b9ec-7d05-4c28-8448-3c0757429b5c",
   "metadata": {},
   "source": [
    " _BatchDataset: It indicates that the dataset returns data in batches.\n",
    " element_spec: This describes the structure of the elements in the dataset.\n",
    " TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name = None): This represents the features, in this case the images, in the dataset. None represents the batch size, which is None here because it can vary depending on how many samples we have in the last batch; 256, 256 represents the height and width of the images; 3 is the number of channels in the images, indicating they are RGB images. Last, dtype=tf.float32 tells us that the data type of the image pixels is a 32-bit floating point.\n",
    " TensorSpec(shape=(None, 3), dtype=tf.float32, name=None): This represents the labels/targets of our dataset. Here, None refers to the batch size; 10 refers to the number of labels in the dataset; whilst dtype=tf.float32 is also a 32-bit floating point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c12478-1029-4e68-ad72-68c252fd197b",
   "metadata": {},
   "source": [
    "By using the image_dataset_from_directory function, we have been able to automatically preprocess some aspects of the data. For instance, all the images are now of the same data type, tf.float32. By setting image_size = (256, 256), we have ensured that all images have the same dimensions, 256×256256×256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117546dc-6b9c-44a2-b502-4f0f2c963f37",
   "metadata": {},
   "source": [
    "To bring the pixel values to the 0 to 1 range, we can easily use one of Keras’ preprocessing layers, tf.keras.layers.Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5b64a9-7f21-4239-a144-46447f59e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Rescaling(1./255) # Defining scaler values between 0 to 1\n",
    "# Rescaling datasets\n",
    "train = train.map(lambda x, y: (scaler(x), y)) \n",
    "#test = test.map(lambda x, y: (scaler(x), y))\n",
    "validation = validation.map(lambda x, y: (scaler(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2ca97-a70c-4bc0-a48d-c270e7e3af4b",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863a717-0cac-4533-9519-ff271393cd72",
   "metadata": {},
   "source": [
    "When working with image data, it is usually a good practice to artificially introduce some diversity to the sample by applying random transformations to the images used in training. This is good because it helps to expose the model to a wider variety of images and avoids overfitting.\n",
    "\n",
    "Keras has about seven different layers for image data augmentation. These are:\n",
    "\n",
    "• tf.keras.layers.RandomCrop: This layer randomly chooses a location to crop images down to a target size.\n",
    "\n",
    "• tf.keras.layers.RandomFlip: This layer randomly flips images horizontally and or vertically based on the mode attribute.\n",
    "\n",
    "• tf.keras.layers.RandomTranslation: This layer randomly applies translations to each image during training according to the fill_mode attribute.\n",
    "\n",
    "• tf.keras.layers.RandomBrightness: This layer randomly increases/reduces the brightness for the input RGB images.\n",
    "\n",
    "• tf.keras.layers.RandomRotation: This layer randomly rotates the images during training, and also fills empty spaces according to the fill_mode attribute.\n",
    "\n",
    "• tf.keras.layers.RandomZoom: This layer randomly zooms in or out on each axis of each image independently during training.\n",
    "\n",
    "• tf.keras.layers.RandomContrast: This layer randomly adjusts contrast by a random factor during training in or out on each axis of each image independently during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c55e51e-9f9d-4f65-8953-0eafb9c9e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data augmentation pipeline\n",
    "augmentation = ImageDataGenerator(\n",
    "    rotation_range=15,  # Random rotation in the range of [-15, 15] degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10% of the width\n",
    "    height_shift_range=0.1,  # Randomly shift images vertically by up to 10% of the height\n",
    "    zoom_range=0.2,  # Randomly zoom into images by up to 20%\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b899f-e888-46fd-8b62-b05b6479eff7",
   "metadata": {},
   "source": [
    "# BUILD CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e69fc9f-bfaa-48ed-94c1-c3e7c16d8e75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initiating model on GPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mstrategy\u001b[49m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(augmentation) \u001b[38;5;66;03m# Adding data augmentation pipeline to the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding data augmentation pipeline to the model\n",
    "model.add(data_augmentation)\n",
    "\n",
    "# Feature Learning Layers\n",
    "model.add(Conv2D(32, (3, 3), strides=1, padding='same', input_shape=(256, 256, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Flattening tensors\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully-Connected Layers\n",
    "model.add(Dense(2048))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))  # 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
